## Predicting code defects using interpretable static measures.

*Authors: Wojciech Bogucki, Tomasz Makowski, Dominik Rafacz (Warsaw University of Technology)*

### Abstract


### Introduction and Motivation

Since the very beginning of the computer revolution there have been attempts to increase efficiency in determining possible defects and failures in the code. An effective method to do so could bring many potential benefits by identifying such sites as early as at the code development stage and eliminating costly errors at the deployment stage. McCabe and Halstead proposed a set of measures that are based on static properties of the code (including basic values, e.g. number of lines of code or number of unique operators, as well as transformations of them, [-@mccabe76, -@halstead77]). In their hypotheses, they argue that these measures can significantly help to build models that predict the sensitive spots in program modules. However, it can be argued that the measures they propose are artificial, non-intuitive, and above all, not necessarily authoritative, not taking into account many aspects of the written code and program [@fenton97].

To support their hypotheses with, McCabe and Halstead collected information about the code used in NASA using scrapers and then used machine learning algorithms. In this article we use the above data sets to build a model that best predicts the vulnerability of the code to errors. We check whether static code measures (being transformations of basic predictors) significantly improve prediction results for the so-called white box models (e.g. trees and linear regression). Our goal is to build, using simple data transformations and easily explainable methods, a so-called white box machine learning model (e.g. tree or logistic regression) that will achieve results comparable to the black box model (such as neural networks or gradient boosting machines) used on data without advanced measures. We also want to compare the effectiveness of the measures proposed by McCabe and Halstead and compare them with the measures we have generated.

### Related Work

### Dataset

Our dataset comes from the original research of Halstead and McCabe. We obtained it by combining the sets from OpenML [@OpenML2013] and supplementing them with data from the PROMISE repository [@Sayyad-Shirabad+Menzies:2005].

It contains data collected from NASA systems written in *C* and *C++* languages. The data is in the form of a data frame containing more than 15000 records. Each record describes one "program module". -- with this generic term, the authors defined the simplest unit of functionality (in this case, these are functions). Each record is described with a set of predictors, which can be divided into several groups:

* Basic measures (such as number of lines of code, number of operands, etc.).
* McCabe's measures how complex the code is in terms of control flow and cross-references .
* Halstead's measures for general code readability.
* Target column (1 if module contains defects, 0 if not).
* Source column we added, specifying from which subsystem the module came (the original 5 datasets came from different systems).

In order to verify our hypotheses, we decided at the beginning to remove the Halstead measures (which were transformations of the basic measures) from the collection to see if we are able to build an effective black box model without them. We also wanted to remove McCabe's measurements, but the basic measurements that he used to calculate his measurements are not preserved in the collection, so we decided to keep them.

### Methodology

Our research consist of the following stages:

1. Data exploration.
2. Initial data preparation.
3. Building of black-box and white-box models and comparing them against the relevant measurements.
4. Repeating the cycle:
   
   (a) Improvement of white box models by modifying their parameters or data.  
   (b) Measuring the effectiveness of the models built.
   (c) Analysis of the resulting models.
   (d) Keeping or rejecting changes for further work.
   
5. Selection of the best white box model and final comparison with the black box model.

We use R programming language [ref] and popular machine learning project management packages - mlr and drake [ref]. 

#### Data exploration

At this stage, we are taking a closer look at what the data looks like and we are analyzing their distributions, gaps, correlations and simple relationships using the DataExplorer and visdat packages [ref].

#### Initial data preparation

This stage consists mainly of merging the data sets, as mentioned earlier, and adding a source column. Since there were very few data gaps, we are imputing them with the median, because this method is effective and fast. [ref]

Imputation is necessary from the very beginning, as many models cannot cope with missing values. Since there were very few missing values, it does not affect significantly the result of those models that models that would still work. We are not carrying out further transformations at this stage because we do not want to disturb the results of the next stage.

#### Starting models

We're building models on this almost unaltered data. We are using the following:

1. Well interpretable models (white-boxes) [ref]:

   * logistic regression
   * the decision tree
   * k-nearest neighbors algorithm
   
2. Poorly interpretable models (black-boxes) [ref]:

   * random forest
   * gradient boosting machine



#### Improving white-boxes

#### Selecting the best model

### Results


### Summary and conclusions