<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Interpretability | ML Case Studies</title>
  <meta name="description" content="Case studies for reproducibility, imputation, and interpretability" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Interpretability | ML Case Studies" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Case studies for reproducibility, imputation, and interpretability" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Interpretability | ML Case Studies" />
  
  <meta name="twitter:description" content="Case studies for reproducibility, imputation, and interpretability" />
  <meta name="twitter:image" content="images/cover.png" />



<meta name="date" content="2020-05-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="imputation.html"/>
<link rel="next" href="acknowledgements.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><h3>ML Case Studies</h3></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="reproducibility.html"><a href="reproducibility.html"><i class="fa fa-check"></i><b>1</b> Reproducibility of scientific papers</a>
<ul>
<li class="chapter" data-level="1.1" data-path="reproducibility.html"><a href="reproducibility.html#title-of-the-article"><i class="fa fa-check"></i><b>1.1</b> Title of the article</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract"><i class="fa fa-check"></i><b>1.1.1</b> Abstract</a></li>
<li class="chapter" data-level="1.1.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation"><i class="fa fa-check"></i><b>1.1.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.1.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work"><i class="fa fa-check"></i><b>1.1.3</b> Related Work</a></li>
<li class="chapter" data-level="1.1.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology"><i class="fa fa-check"></i><b>1.1.4</b> Methodology</a></li>
<li class="chapter" data-level="1.1.5" data-path="reproducibility.html"><a href="reproducibility.html#results"><i class="fa fa-check"></i><b>1.1.5</b> Results</a></li>
<li class="chapter" data-level="1.1.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions"><i class="fa fa-check"></i><b>1.1.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="reproducibility.html"><a href="reproducibility.html#how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers"><i class="fa fa-check"></i><b>1.2</b> How to measure reproducibility? Classification of problems with reproducing scientific papers</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-1"><i class="fa fa-check"></i><b>1.2.1</b> Abstract</a></li>
<li class="chapter" data-level="1.2.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction"><i class="fa fa-check"></i><b>1.2.2</b> Introduction</a></li>
<li class="chapter" data-level="1.2.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-1"><i class="fa fa-check"></i><b>1.2.3</b> Related Work</a></li>
<li class="chapter" data-level="1.2.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-1"><i class="fa fa-check"></i><b>1.2.4</b> Methodology</a></li>
<li class="chapter" data-level="1.2.5" data-path="reproducibility.html"><a href="reproducibility.html#results-1"><i class="fa fa-check"></i><b>1.2.5</b> Results</a></li>
<li class="chapter" data-level="1.2.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-1"><i class="fa fa-check"></i><b>1.2.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="reproducibility.html"><a href="reproducibility.html#aging-articles.-how-time-affects-reproducibility-of-scientific-papers"><i class="fa fa-check"></i><b>1.3</b> Aging articles. How time affects reproducibility of scientific papers?</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-2"><i class="fa fa-check"></i><b>1.3.1</b> Abstract</a></li>
<li class="chapter" data-level="1.3.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-1"><i class="fa fa-check"></i><b>1.3.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3.3" data-path="reproducibility.html"><a href="reproducibility.html#methodology-2"><i class="fa fa-check"></i><b>1.3.3</b> Methodology</a></li>
<li class="chapter" data-level="1.3.4" data-path="reproducibility.html"><a href="reproducibility.html#results-2"><i class="fa fa-check"></i><b>1.3.4</b> Results</a></li>
<li class="chapter" data-level="1.3.5" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-2"><i class="fa fa-check"></i><b>1.3.5</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="reproducibility.html"><a href="reproducibility.html#ways-to-reproduce-articles-in-terms-of-release-date-and-magazine"><i class="fa fa-check"></i><b>1.4</b> Ways to reproduce articles in terms of release date and magazine</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-3"><i class="fa fa-check"></i><b>1.4.1</b> Abstract</a></li>
<li class="chapter" data-level="1.4.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-1"><i class="fa fa-check"></i><b>1.4.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.4.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-2"><i class="fa fa-check"></i><b>1.4.3</b> Related Work</a></li>
<li class="chapter" data-level="1.4.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-3"><i class="fa fa-check"></i><b>1.4.4</b> Methodology</a></li>
<li class="chapter" data-level="1.4.5" data-path="reproducibility.html"><a href="reproducibility.html#results-3"><i class="fa fa-check"></i><b>1.4.5</b> Results</a></li>
<li class="chapter" data-level="1.4.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-3"><i class="fa fa-check"></i><b>1.4.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="reproducibility.html"><a href="reproducibility.html#reproducibility-of-outdated-articles-about-up-to-date-r-packages"><i class="fa fa-check"></i><b>1.5</b> Reproducibility of outdated articles about up-to-date R packages</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-4"><i class="fa fa-check"></i><b>1.5.1</b> Abstract</a></li>
<li class="chapter" data-level="1.5.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-2"><i class="fa fa-check"></i><b>1.5.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.5.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-3"><i class="fa fa-check"></i><b>1.5.3</b> Related Work</a></li>
<li class="chapter" data-level="1.5.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-4"><i class="fa fa-check"></i><b>1.5.4</b> Methodology</a></li>
<li class="chapter" data-level="1.5.5" data-path="reproducibility.html"><a href="reproducibility.html#results-4"><i class="fa fa-check"></i><b>1.5.5</b> Results</a></li>
<li class="chapter" data-level="1.5.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-4"><i class="fa fa-check"></i><b>1.5.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="reproducibility.html"><a href="reproducibility.html#correlation-between-reproducibility-of-components-of-research-papers-and-their-purpose"><i class="fa fa-check"></i><b>1.6</b> Correlation between reproducibility of components of research papers and their purpose</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-5"><i class="fa fa-check"></i><b>1.6.1</b> Abstract</a></li>
<li class="chapter" data-level="1.6.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-3"><i class="fa fa-check"></i><b>1.6.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.6.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-4"><i class="fa fa-check"></i><b>1.6.3</b> Related Work</a></li>
<li class="chapter" data-level="1.6.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-5"><i class="fa fa-check"></i><b>1.6.4</b> Methodology</a></li>
<li class="chapter" data-level="1.6.5" data-path="reproducibility.html"><a href="reproducibility.html#results-5"><i class="fa fa-check"></i><b>1.6.5</b> Results</a></li>
<li class="chapter" data-level="1.6.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-5"><i class="fa fa-check"></i><b>1.6.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="reproducibility.html"><a href="reproducibility.html#how-active-development-affects-reproducibility"><i class="fa fa-check"></i><b>1.7</b> How active development affects reproducibility</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-6"><i class="fa fa-check"></i><b>1.7.1</b> Abstract</a></li>
<li class="chapter" data-level="1.7.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-4"><i class="fa fa-check"></i><b>1.7.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.7.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-5"><i class="fa fa-check"></i><b>1.7.3</b> Related Work</a></li>
<li class="chapter" data-level="1.7.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-6"><i class="fa fa-check"></i><b>1.7.4</b> Methodology</a></li>
<li class="chapter" data-level="1.7.5" data-path="reproducibility.html"><a href="reproducibility.html#results-6"><i class="fa fa-check"></i><b>1.7.5</b> Results</a></li>
<li class="chapter" data-level="1.7.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-6"><i class="fa fa-check"></i><b>1.7.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="reproducibility.html"><a href="reproducibility.html#reproducibility-differences-of-articles-published-in-various-journals-and-using-r-or-python-language"><i class="fa fa-check"></i><b>1.8</b> Reproducibility differences of articles published in various journals and using R or Python language</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-7"><i class="fa fa-check"></i><b>1.8.1</b> Abstract</a></li>
<li class="chapter" data-level="1.8.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-5"><i class="fa fa-check"></i><b>1.8.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.8.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-6"><i class="fa fa-check"></i><b>1.8.3</b> Related Work</a></li>
<li class="chapter" data-level="1.8.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-7"><i class="fa fa-check"></i><b>1.8.4</b> Methodology</a></li>
<li class="chapter" data-level="1.8.5" data-path="reproducibility.html"><a href="reproducibility.html#results-7"><i class="fa fa-check"></i><b>1.8.5</b> Results</a></li>
<li class="chapter" data-level="1.8.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-7"><i class="fa fa-check"></i><b>1.8.6</b> Summary and conclusions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="imputation.html"><a href="imputation.html"><i class="fa fa-check"></i><b>2</b> Imputation</a></li>
<li class="chapter" data-level="3" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>3</b> Interpretability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="interpretability.html"><a href="interpretability.html#building-an-explainable-model-for-ordinal-classification.-meeting-black-box-model-performance-levels."><i class="fa fa-check"></i><b>3.1</b> Building an explainable model for ordinal classification. Meeting black box model performance levels.</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="interpretability.html"><a href="interpretability.html#abstract-8"><i class="fa fa-check"></i><b>3.1.1</b> Abstract</a></li>
<li class="chapter" data-level="3.1.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-6"><i class="fa fa-check"></i><b>3.1.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.1.3" data-path="interpretability.html"><a href="interpretability.html#related-work-7"><i class="fa fa-check"></i><b>3.1.3</b> Related Work</a></li>
<li class="chapter" data-level="3.1.4" data-path="interpretability.html"><a href="interpretability.html#methodology-8"><i class="fa fa-check"></i><b>3.1.4</b> Methodology</a></li>
<li class="chapter" data-level="3.1.5" data-path="interpretability.html"><a href="interpretability.html#model-explanantion"><i class="fa fa-check"></i><b>3.1.5</b> Model explanantion</a></li>
<li class="chapter" data-level="3.1.6" data-path="interpretability.html"><a href="interpretability.html#results-8"><i class="fa fa-check"></i><b>3.1.6</b> Results</a></li>
<li class="chapter" data-level="3.1.7" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-8"><i class="fa fa-check"></i><b>3.1.7</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="interpretability.html"><a href="interpretability.html#predicting-code-defects-using-interpretable-static-measures."><i class="fa fa-check"></i><b>3.2</b> Predicting code defects using interpretable static measures.</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="interpretability.html"><a href="interpretability.html#abstract-9"><i class="fa fa-check"></i><b>3.2.1</b> Abstract</a></li>
<li class="chapter" data-level="3.2.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-7"><i class="fa fa-check"></i><b>3.2.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.2.3" data-path="interpretability.html"><a href="interpretability.html#dataset"><i class="fa fa-check"></i><b>3.2.3</b> Dataset</a></li>
<li class="chapter" data-level="3.2.4" data-path="interpretability.html"><a href="interpretability.html#methodology-9"><i class="fa fa-check"></i><b>3.2.4</b> Methodology</a></li>
<li class="chapter" data-level="3.2.5" data-path="interpretability.html"><a href="interpretability.html#results-9"><i class="fa fa-check"></i><b>3.2.5</b> Results</a></li>
<li class="chapter" data-level="3.2.6" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-9"><i class="fa fa-check"></i><b>3.2.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="interpretability.html"><a href="interpretability.html#using-interpretable-machine-learning-models-in-the-higgs-boson-detection."><i class="fa fa-check"></i><b>3.3</b> Using interpretable Machine Learning models in the Higgs boson detection.</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="interpretability.html"><a href="interpretability.html#abstract-10"><i class="fa fa-check"></i><b>3.3.1</b> Abstract</a></li>
<li class="chapter" data-level="3.3.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-8"><i class="fa fa-check"></i><b>3.3.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.3.3" data-path="interpretability.html"><a href="interpretability.html#related-work-8"><i class="fa fa-check"></i><b>3.3.3</b> Related Work</a></li>
<li class="chapter" data-level="3.3.4" data-path="interpretability.html"><a href="interpretability.html#methodology-10"><i class="fa fa-check"></i><b>3.3.4</b> Methodology</a></li>
<li class="chapter" data-level="3.3.5" data-path="interpretability.html"><a href="interpretability.html#results-10"><i class="fa fa-check"></i><b>3.3.5</b> Results</a></li>
<li class="chapter" data-level="3.3.6" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-10"><i class="fa fa-check"></i><b>3.3.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="interpretability.html"><a href="interpretability.html#optimizing-features-transformations-for-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Optimizing features’ transformations for linear regression</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="interpretability.html"><a href="interpretability.html#abstract-11"><i class="fa fa-check"></i><b>3.4.1</b> Abstract</a></li>
<li class="chapter" data-level="3.4.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-9"><i class="fa fa-check"></i><b>3.4.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.4.3" data-path="interpretability.html"><a href="interpretability.html#related-work-9"><i class="fa fa-check"></i><b>3.4.3</b> Related Work</a></li>
<li class="chapter" data-level="3.4.4" data-path="interpretability.html"><a href="interpretability.html#methodology-11"><i class="fa fa-check"></i><b>3.4.4</b> Methodology</a></li>
<li class="chapter" data-level="3.4.5" data-path="interpretability.html"><a href="interpretability.html#results-11"><i class="fa fa-check"></i><b>3.4.5</b> Results</a></li>
<li class="chapter" data-level="3.4.6" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-11"><i class="fa fa-check"></i><b>3.4.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="interpretability.html"><a href="interpretability.html#surpassing-black-box-models-performance-on-unbalanced-data-with-an-interpretable-one."><i class="fa fa-check"></i><b>3.5</b> Surpassing black box model’s performance on unbalanced data with an interpretable one.</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="interpretability.html"><a href="interpretability.html#abstract-12"><i class="fa fa-check"></i><b>3.5.1</b> Abstract</a></li>
<li class="chapter" data-level="3.5.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-10"><i class="fa fa-check"></i><b>3.5.2</b> Introduction and Motivation</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="interpretability.html"><a href="interpretability.html#which-neighbours-affected-the-price-of-a-house-in-the-90s"><i class="fa fa-check"></i><b>3.6</b> Which Neighbours Affected the Price of a House in the ’90s?</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="interpretability.html"><a href="interpretability.html#introduction-2"><i class="fa fa-check"></i><b>3.6.1</b> Introduction</a></li>
<li class="chapter" data-level="3.6.2" data-path="interpretability.html"><a href="interpretability.html#data"><i class="fa fa-check"></i><b>3.6.2</b> Data</a></li>
<li class="chapter" data-level="3.6.3" data-path="interpretability.html"><a href="interpretability.html#methodology-12"><i class="fa fa-check"></i><b>3.6.3</b> Methodology</a></li>
<li class="chapter" data-level="3.6.4" data-path="interpretability.html"><a href="interpretability.html#results-12"><i class="fa fa-check"></i><b>3.6.4</b> Results</a></li>
<li class="chapter" data-level="3.6.5" data-path="interpretability.html"><a href="interpretability.html#conclusions"><i class="fa fa-check"></i><b>3.6.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="interpretability.html"><a href="interpretability.html#explainable-computer-vision-with-embeddings-and-knn-classifier."><i class="fa fa-check"></i><b>3.7</b> Explainable Computer Vision with embeddings and KNN classifier.</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="interpretability.html"><a href="interpretability.html#abstract-13"><i class="fa fa-check"></i><b>3.7.1</b> Abstract</a></li>
<li class="chapter" data-level="3.7.2" data-path="interpretability.html"><a href="interpretability.html#introduction-3"><i class="fa fa-check"></i><b>3.7.2</b> Introduction</a></li>
<li class="chapter" data-level="3.7.3" data-path="interpretability.html"><a href="interpretability.html#data-1"><i class="fa fa-check"></i><b>3.7.3</b> Data</a></li>
<li class="chapter" data-level="3.7.4" data-path="interpretability.html"><a href="interpretability.html#methodology-13"><i class="fa fa-check"></i><b>3.7.4</b> Methodology</a></li>
<li class="chapter" data-level="3.7.5" data-path="interpretability.html"><a href="interpretability.html#results-13"><i class="fa fa-check"></i><b>3.7.5</b> Results</a></li>
<li class="chapter" data-level="3.7.6" data-path="interpretability.html"><a href="interpretability.html#conclusions-1"><i class="fa fa-check"></i><b>3.7.6</b> Conclusions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>4</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ML Case Studies</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interpretability" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Interpretability</h1>
<p>Interpretability</p>

<div id="building-an-explainable-model-for-ordinal-classification.-meeting-black-box-model-performance-levels." class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Building an explainable model for ordinal classification. Meeting black box model performance levels.</h2>
<p><em>Authors: Karol Saputa, Małgorzata Wachulec, Aleksandra Wichrowska (Warsaw University of Technology)</em></p>
<div id="abstract-8" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-6" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Introduction and Motivation</h3>
<p>In the classification problems, the main goal is to map inputs to a categorical target variable. Most machine learning algorithms assume that the class attribute is unordered. However, there are many problems where target variable is ranked, for example while predicting movie ratings. When applying standard methods to such problems, we lose some informaton, which could improve our model performance.</p>
<p>This paper presents various methods to make an explainable machine learning model for ordinal classification problem. The aim is to achieve better results than ‘black box’ model does. We will test some existing approaches to ordinal classification and take advantage of analysis and imputation of missing data, feature transformation and selection, as well as knowledge from exploratory data analysis.</p>
<p>Our experiments are based on ‘eucalyptus’ dataset from OpenML. The dataset’s objective is to find the best seedlot for soil conservation in seasonally dry hill country. Predictions are made depending on features such as height, diameter and survival of the plants. Target variable is ordered - it is represented by values ‘low’, ‘average’, ‘good’ and ‘best’.</p>
</div>
<div id="related-work-7" class="section level3" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Related Work</h3>
<div id="an-approach-to-ordinal-classification" class="section level4" number="3.1.3.1">
<h4><span class="header-section-number">3.1.3.1</span> An approach to ordinal classification</h4>
</div>
<div id="comparing-black-box-to-linear-model" class="section level4" number="3.1.3.2">
<h4><span class="header-section-number">3.1.3.2</span> Comparing black box to linear model</h4>
</div>
</div>
<div id="methodology-8" class="section level3" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Methodology</h3>
<div id="initial-preprocessing" class="section level4" number="3.1.4.1">
<h4><span class="header-section-number">3.1.4.1</span> Initial preprocessing</h4>
<p>The aim of this article was to build the best interpretable model for an ordinal classification problem and comparing it to a black box model. First undertaken step was dividing the data into training and test sets, consisting of 70% and 30% of all data, respectively. Since the considered dataset has a categorical target variable, the division was done using random sampling within each class of the target variable in an attempt to balance the class distributions within the splits. A seed was used to assure the same split in each tested model.</p>
<p>In order to get a legitimate comparison, the data was initially preprocessed in such a way as to assure that both models’ performances are compared on the same test set. This initial preprocessing included:</p>
<ol style="list-style-type: decimal">
<li><p>deleting the observations with ‘none’ value in the target variable from both the training set and the test set;</p></li>
<li><p>deleting observations with missing values from test set, resulting in a 6% decease of the test set observations.</p></li>
</ol>
<p>It is important to note that missing values are still present in the training set.</p>
<p>The reason why the missing data is deleted from the test set is that many of the explainable models cannot be run with missing data present. This means that the missing values will have to either be deleted or imputed later on. This leads to a possibility that the explainable model will impute missing data differently than the black box model, resulting in two different tests sets. And, if - instead of imputing missing data - we decide to delete it in order to make running explainable model possible, then the obtained test sets will differ in number of rows, making it impossible to draw any meaningful conclusions. Hence the missing data were deleted from the test set.</p>
</div>
<div id="running-the-black-box-model" class="section level4" number="3.1.4.2">
<h4><span class="header-section-number">3.1.4.2</span> Running the black box model</h4>
<p>The black box model chosen for comparison is an extreme gradient boosting model. After the initial preprocessing the xgboost model was trained on the training set and used to predict results on the test set. As this model can only deal with numerical data, categorical (factor) variables were transformed using one hot encoding. The training proces and prediction were done using the mlr package in R, and the exact model specifications were the following:</p>
<ul>
<li>Learner classif.xgboost from package xgboost</li>
<li>Type: classif</li>
<li>Name: eXtreme Gradient Boosting; Short name: xgboost</li>
<li>Class: classif.xgboost</li>
<li>Properties: twoclass,multiclass,numerics,prob,weights,missings,featimp</li>
<li>Predict-Type: response</li>
<li>Hyperparameters: nrounds=200,verbose=0,objective=multi:softmax</li>
</ul>
<p>The quality of prediction was measured using the AUC (area under ROC curve) measure. This provides the base for this research, to which other models’ results will be compared to.</p>
</div>
<div id="running-the-basic-version-the-explainable-model" class="section level4" number="3.1.4.3">
<h4><span class="header-section-number">3.1.4.3</span> Running the basic version the explainable model</h4>
<p>We have chosen a tree model to be the considered explainable model, its exact specifications were the following:</p>
<ul>
<li>Learner classif.rpart from package rpart</li>
<li>Type: classif</li>
<li>Name: Decision Tree; Short name: rpart</li>
<li>Class: classif.rpart</li>
<li>Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp</li>
<li>Predict-Type: response</li>
<li>Hyperparameters: xval=0</li>
</ul>
<p>As this model cannot be run with missing data, they were deleted from the training set before training the model. Another step was deleting one from each of the one-hot-encoded variables (the default function transforms variable with n factor levels into n columns, but n-1 columns are sufficient as the n-th column is a linear combination of the remaining n-1 columns). This model performed worse than the black box model - the outcomes are presented in the Results section of this article.</p>
</div>
<div id="improving-the-explainable-model" class="section level4" number="3.1.4.4">
<h4><span class="header-section-number">3.1.4.4</span> Improving the explainable model</h4>
<p>As mentioned before, the explainable model was enhanced by applying existing approaches to ordinal classification, feature transformation and selection and missing data imputation. The refinement process consisted of, but was not limited to, the following:</p>
<ol style="list-style-type: decimal">
<li>Transforming Latitude variable from factor to numeric.</li>
<li>Splitting a multiclass classification problem into 3 binary classification problems, like explained in the An approach to ordinal classification section of this article, and using the rpart model on each of the binary problems.</li>
<li>Selecting variables: deleting the site names and specific location tags.</li>
<li>Imputing missing data in the training set.</li>
<li>Tunning the model.</li>
</ol>
<p>The third step has a scientific justification. The experiment for which the data was collected was focused on finding the best seedlot for soil conservation in seasonally dry hill country. All the data in this dataset comes from New Zealand, but there is a chance that the results of such experiment would be used for other geographical regions. So far our model was making the prediction based also on specific flat map coordinates and site names, that are present both in the training and the test set. This means it would be impossible to use this model for judging seedlots of eucalypti planted outside of New Zealand. To make this possible, we have decided to take away all the variables that give away the exact position of the seedlots, leaving features such as latitude and the characteristics of plants and their habitat.</p>
<p>After each improvement the model was retrained and the results obtained on the test set were saved and compared with the previous version of the model. If the new change has improved the model’s performance on the test set then it became the base for further development. Instead, if it has not improved the model’s performance, the previous version of the model was being further developed.</p>
</div>
</div>
<div id="model-explanantion" class="section level3" number="3.1.5">
<h3><span class="header-section-number">3.1.5</span> Model explanantion</h3>
</div>
<div id="results-8" class="section level3" number="3.1.6">
<h3><span class="header-section-number">3.1.6</span> Results</h3>
</div>
<div id="summary-and-conclusions-8" class="section level3" number="3.1.7">
<h3><span class="header-section-number">3.1.7</span> Summary and conclusions</h3>

</div>
</div>
<div id="predicting-code-defects-using-interpretable-static-measures." class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Predicting code defects using interpretable static measures.</h2>
<p><em>Authors: Wojciech Bogucki, Tomasz Makowski, Dominik Rafacz (Warsaw University of Technology)</em></p>
<div id="abstract-9" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-7" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Introduction and Motivation</h3>
<p>Since the very beginning of the computer revolution there have been attempts to increase efficiency in determining possible defects and failures in the code. An effective method to do so could bring many potential benefits by identifying such sites as early as at the code development stage and eliminating costly errors at the deployment stage. McCabe and Halstead proposed a set of measures that are based on static properties of the code (including basic values, e.g. number of lines of code or number of unique operators, as well as transformations of them, <span class="citation">(<a href="#ref-mccabe76" role="doc-biblioref">1976</a>, @halstead77)</span>). In their hypotheses, they argue that these measures can significantly help to build models that predict the sensitive spots in program modules. However, it can be argued that the measures they propose are artificial, non-intuitive, and above all, not necessarily authoritative, not taking into account many aspects of the written code and program <span class="citation">(Fenton and Pfleeger <a href="#ref-fenton97" role="doc-biblioref">1997</a>)</span>.</p>
<p>To support their hypotheses with, McCabe and Halstead collected information about the code used in NASA using scrapers and then used machine learning algorithms. In this article we use the above data sets to build a model that best predicts the vulnerability of the code to errors. We check whether static code measures (being transformations of basic predictors) significantly improve prediction results for the so-called white box models (e.g. trees and linear regression). Our goal is to build, using simple data transformations and easily explainable methods, a so-called white box machine learning model (e.g. tree or logistic regression) that will achieve results comparable to the black box model (such as neural networks or gradient boosting machines) used on data without advanced measures. We also want to compare the effectiveness of the measures proposed by McCabe and Halstead and compare them with the measures we have generated.</p>
</div>
<div id="dataset" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Dataset</h3>
<p>Our dataset comes from the original research of Halstead and McCabe. We obtained it by combining the sets from OpenML <span class="citation">(Vanschoren et al. <a href="#ref-OpenML2013" role="doc-biblioref">2013</a>)</span> and supplementing them with data from the PROMISE repository <span class="citation">(Sayyad Shirabad and Menzies <a href="#ref-Sayyad-Shirabad+Menzies:2005" role="doc-biblioref">2005</a>)</span>.</p>
<p>It contains data collected from NASA systems written in <em>C</em> and <em>C++</em> languages. The data is in the form of a data frame containing more than 15000 records. Each record describes one “program module”. – with this generic term, the authors defined the simplest unit of functionality (in this case, these are functions). Each record is described with a set of predictors, which can be divided into several groups:</p>
<ul>
<li>Basic measures (such as number of lines of code, number of operands, etc.).</li>
<li>McCabe’s measures how complex the code is in terms of control flow and cross-references .</li>
<li>Halstead’s measures for general code readability.</li>
<li>Target column (1 if module contains defects, 0 if not).</li>
<li>Source column we added, specifying from which subsystem the module came (the original 5 datasets came from different systems).</li>
</ul>
<p>In order to verify our hypotheses, we decided at the beginning to remove the Halstead measures (which were transformations of the basic measures) from the collection to see if we are able to build an effective black box model without them. We also wanted to remove McCabe’s measurements, but the basic measurements that he used to calculate his measurements are not preserved in the collection, so we decided to keep them.</p>
<p>There are not many records with openly missing data in the set (&lt; 1%), however, the values of some columns raise doubts – in the column containing information about the number of lines of code of a given module in many cases there is a value 0, which is not reliable. We consider 0 in this column to be missing data and remove records that have 0 here (almost 2000 records).</p>
</div>
<div id="methodology-9" class="section level3" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Methodology</h3>
<p>Our research consist of the following stages:</p>
<ol style="list-style-type: decimal">
<li><p>Data exploration.</p></li>
<li><p>Initial data preparation.</p></li>
<li><p>Building of black-box and white-box models and comparing them against the relevant measurements.</p></li>
<li><p>Repeating the cycle:</p>
<ol style="list-style-type: lower-alpha">
<li>Improvement of white box models by modifying their parameters or data.<br />
</li>
<li>Measuring the effectiveness of the models built.</li>
<li>Analysis of the resulting models.</li>
<li>Keeping or rejecting changes for further work.</li>
</ol></li>
<li><p>Selection of the best white box model and final comparison with the black box model.</p></li>
</ol>
<p>We use R programming language [ref] and popular machine learning project management packages - mlr and drake [ref].</p>
<div id="data-exploration" class="section level4" number="3.2.4.1">
<h4><span class="header-section-number">3.2.4.1</span> Data exploration</h4>
<p>At this stage, we are taking a closer look at what the data looks like and we are analyzing their distributions, gaps, correlations and simple relationships using the DataExplorer and visdat packages [ref].</p>
</div>
<div id="initial-data-preparation" class="section level4" number="3.2.4.2">
<h4><span class="header-section-number">3.2.4.2</span> Initial data preparation</h4>
<p>This stage consists mainly of merging the data sets, as mentioned earlier, and adding a source column (in fact, we added five indicator columns, which contain one-hot-encoded value, as models generally do not cope well with character columns). Since there were very few data gaps, we are imputing them with the median, because this method is effective and fast. [ref]</p>
<p>Imputation is necessary from the very beginning, as many models cannot cope with missing values. Since there were very few missing values, it does not affect significantly the result of those models that models that would still work. We are not carrying out further transformations at this stage because we do not want to disturb the results of the next stage.</p>
</div>
<div id="starting-models" class="section level4" number="3.2.4.3">
<h4><span class="header-section-number">3.2.4.3</span> Starting models</h4>
<p>We’re building models on this almost unaltered data. We are using one poorly interpretable model (black-box) – random forest, specifically ranger package [ref], because it is fast and low-effort. Among well interpretable models (white-boxes) used in our work there are:</p>
<ul>
<li>logistic regression [ref]</li>
<li>decision tree [ref]</li>
<li>k-nearest neighbors algorithm [ref]</li>
</ul>
<p>We train the models into data that we have divided into five folds with a similar distribution of the decision variable, on which we will perform cross-validation. Then we compare the results using commonly used measure – AUC (Area Under Curve) [ref], which not only assesses whether the observations are well classified, but also takes into account the likelihood of belonging to a class. We use AUC as the main comparative criterion of the models also in the further part of our work.</p>
</div>
<div id="improving-white-boxes" class="section level4" number="3.2.4.4">
<h4><span class="header-section-number">3.2.4.4</span> Improving white-boxes</h4>
<p>This is a key part of our work. In the iterative cycle we use different methods to improve the quality of the white box models. After applying each of these methods, we check whether it has improved our score and possibly analyze the model, using statistical methods (residuals analysis) and explanatory machine learning (DALEX package [ref]), to draw indications of what should be done next.</p>
<p>We are trying the following methods:</p>
<ul>
<li><strong>Logarithmic and exponential transformations of individual variables</strong> – So that linear relationships can be better captured and to reduce the influence of outliers, we transform variables using exponential and polynomial functions.</li>
<li><strong>Discretization of continuous features</strong> – Some variables do not have a linear effect on the response variable, even if they are transformed by simple functions like exponential function, sometimes there are clear thresholds – so we can replace the variable with indexes of individual segments. The rSAFE package helps with this [ref].</li>
<li><strong>Generating new columns as functions of other columns</strong> – There may be interactions between variables that cannot be captured by linear models. In order to take them into account, we generate new columns, applying to the rest of them various transformations – we take their inverses, products, quotients, elevations to power, and so on. As a result of these operations, a lot of new predictors are created, which we later evaluate. We also analyze their interpretability, i.e. to what extent they are translatable into an intuitive understanding of such a measure. At this point we also consider Halstead and McCabe’s measurements.</li>
<li><strong>Oversampling and undersampling</strong> – On the basis of the data set, we generate more observations from the minority class using the SMOTE algorithm [ref] and remove some of the observations from the majority class so that the model more emphasizes the differences in characteristics of individual classes.</li>
</ul>
</div>
<div id="selecting-the-best-model" class="section level4" number="3.2.4.5">
<h4><span class="header-section-number">3.2.4.5</span> Selecting the best model</h4>
<p>At the end of the process, we select the model that has the highest AUC score for crossvlidation on our dataset.</p>
</div>
</div>
<div id="results-9" class="section level3" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Results</h3>
</div>
<div id="summary-and-conclusions-9" class="section level3" number="3.2.6">
<h3><span class="header-section-number">3.2.6</span> Summary and conclusions</h3>

</div>
</div>
<div id="using-interpretable-machine-learning-models-in-the-higgs-boson-detection." class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Using interpretable Machine Learning models in the Higgs boson detection.</h2>
<p><em>Authors: Mateusz Bakala, Michal Pastuszka, Karol Pysiak (Warsaw University of Technology)</em></p>
<div id="abstract-10" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-8" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Introduction and Motivation</h3>
</div>
<div id="related-work-8" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Related Work</h3>
</div>
<div id="methodology-10" class="section level3" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Methodology</h3>
</div>
<div id="results-10" class="section level3" number="3.3.5">
<h3><span class="header-section-number">3.3.5</span> Results</h3>
</div>
<div id="summary-and-conclusions-10" class="section level3" number="3.3.6">
<h3><span class="header-section-number">3.3.6</span> Summary and conclusions</h3>

</div>
</div>
<div id="optimizing-features-transformations-for-linear-regression" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Optimizing features’ transformations for linear regression</h2>
<p><em>Authors: Łukasz Brzozowski, Wojciech Kretowicz, Kacper Siemaszko (Warsaw University of Technology)</em></p>
<div id="abstract-11" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-9" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Introduction and Motivation</h3>
<p>Linear regression is one of the simplest and the easiest to interpret of the predictive models. While it has already been thorougly analysed over the years (ref), there remain some unsolved questions. One such question is how to transform the data features in order to maximize the model’s effectiveness in predicting the new data. An example of a known and widely used approach is the Box-Cox transformation of the target variable, which allows one to improve the model’s performance with minimal increase in computational complexity. However, choice of the predictive features’ transformations is often left to intuition and trial-and-error approach. In the article, we wish to compare various methods of features’ transformations and compare the resulting models’ performances while also comparing their computational complexities and differences in feature importance.
Many black box regression models use various kinds of feature engineering during the training process. Unfortunately, even though the models perform better than the interpretable ones, they don’t provide information about the transformations used and non-linear dependencies between variables and the target. The goal we want to achieve is extracting features and non-linearities with understandable transformations of the training dataset. To measure the improvement of used methods, we’ll compare their performance metrics with black box models’ as a ground truth. This will allow us to effectively measure which method brought the simple linear model closer to the black box. Also, we’ll take under consideration the improvement of black box model performance. Thanks to this, our article will not only present the methods for creating highly performant interpretable models, but also improvement of the results of black box model.</p>
<p>//cytowania</p>
</div>
<div id="related-work-9" class="section level3" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Related Work</h3>
<p>There exist many papers related to feature engineering. We will shortly present few of them.</p>
<p>One of these papers is “Enhancing Regression Models for Complex Systems Using Evolutionary Techniques for Feature Engineering” Patricia Arroba, José L. Risco-Martín, Marina Zapater, José M. Moya &amp; José L. Ayala. This paper describes, how feature transformations in linear regression can be chosen based on the genetic algorithms.</p>
<p>Another one is “Automatic feature engineering for regression models with machine learning: An evolutionary computation and statistics hybrid” Vinícius Veloso de Melo, Wolfgang Banzhaf. Similarly to the previous one, this paper tries to automate feature engineering using evolutionar computation to make a hybrid model - final model is simple linear regression while its features are found by more complex algorithm.</p>
<p>//cytowania</p>
</div>
<div id="methodology-11" class="section level3" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Methodology</h3>
<p>The main goal of our research is to compare various methods of transforming the data in order to improve the linear regression’s performance. While we do not aim to derive an explicitly best solution to the problem, we wish to compare some known approaches and propose new ones, simultaneously verifying legitimacy of their usage. The second goal of the research is to compare the achieved models’ performances with black box models to generally compare their effectiveness, having in mind that the linear regression remains highly interpretable. We also wish to compare the models feature importance to check for notable differences.</p>
<p>The main methods of feature transformation compared in the article include:</p>
<ol style="list-style-type: decimal">
<li><p>By-hand-transformations - we will use our expertise to derive possibly the best transformations of the dataset, but in this scenario we do not automate the process. Based on various visualizations, including, but not limited to residual plots, Feature Importance plots, Partial Dependency plots and Accumulated Dependency plots, we aim to maximize the model’s performance by hand.</p></li>
<li><p>Brute Force method - this method of data transformation generates huge amount of additional features being transformations of the existing features. They include e.g. taking square of the variable value or multiplying two variables. While the method is known to provide good results, its complexity is much higher than other methods’ and may lead to overfitting.</p></li>
<li><p>Bayesian Optimization method - we wish to treat the task of finding optimal data transformation as an optimization problem. Once we restrict the transformations e.g. by choosing maximal degree of a polynomial transformation of each variable, we may then search the possible models’ space with the use of Bayesian Optimization in order to maximize their performance. This way we may also restrain the model from generating a lrage number of variables, while also hopefully yielding good results.</p></li>
<li><p>One of our ideas is to use GP (Genetic Programming) to find best feature transformations. We will create a set of simple operations such as adding, multiplying, taking a specific power, for example 2, taking logarithm and so on. Our goal is to minimize mean squared error of linear regression (ridge) after transformations. We will use one of the variations of the genetic algorithms to create an operation tree minimizing our goal. This method should find much better solutions without extending dimensionality too much or making too complex transformations. We will get linear regression with much better performance without loss of the interpretability. This method will teach linear regression many times, because in each generation each individual requires its own training. However, linear regressions are very fast even for datasets with many rows and many columns, thus computation complexity should not be a problem. Whole conception tries to automate feature enginnering done traditionally by hand. Another advantage is control of model complexity. We can stimulate how the operation trees are made, and reduce or increase complexity at will. Modification of this idea is to add regularization term decreasing survival probability with increasing complexity. At the end model could also make a feature selection in the same way - then one of possible operations in the set would be dropping.</p></li>
</ol>
<p>The research is conducted on <em>Concrete_Data</em> dataset from the OpenML database. The data describes the amount of ingredients in the samples - cement, blast furnace slag, fly ash, water, coarse aggregate and fine aggregate - in kilograms per cubic meter; it also contains the drying time of the samples in days, referred to as age. The target variable of the dataset is the compressive strength of each sample in megapascals (MPa), therefore rendering the task to be regressive. The dataset contains 1030 instances with no missing values. There are also no symbolic features, as we aim to investigate continuous transformations of the data.</p>
<p>We use standard and verified methods to compare results of the models. As the target variable is continuous, we may calculate Mean Square Error (MSE), Mean Absolute Error (MAE), and R-squared measures for each model, which provides us with proper and measurable way to compare the models’ performances. The same measures may be applied to black box models. The feature importance measure used in the after-evaluation comparison is based on the permutation feature importance, easily applied to any predictive machine learning model and therefore not constraining us to choose from a restricted set. In order provide unbiased results, we calculate the measures’ values during cross-validation process for each model, using various number of fold to present comparative results.</p>
<p>//cytowania do dopisania</p>
</div>
<div id="results-11" class="section level3" number="3.4.5">
<h3><span class="header-section-number">3.4.5</span> Results</h3>
</div>
<div id="summary-and-conclusions-11" class="section level3" number="3.4.6">
<h3><span class="header-section-number">3.4.6</span> Summary and conclusions</h3>

</div>
</div>
<div id="surpassing-black-box-models-performance-on-unbalanced-data-with-an-interpretable-one." class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Surpassing black box model’s performance on unbalanced data with an interpretable one.</h2>
<p><em>Authors: Witold Merkel, Adam Rydelek, Michał Stawikowski (Warsaw University of Technology)</em></p>
<div id="abstract-12" class="section level3" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-10" class="section level3" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Introduction and Motivation</h3>
<p>Recently, an increase in demand of interpretable models can be seen. Machine learning models have gained in popularity in recent years among many fields of business science, industry and also more and more often in medicine.
“Interpretability is a quickly growing field in machine learning, and there have been multiple works examining various aspects of interpretations (sometimes under the heading explainable AI).” <span class="citation">(Murdoch <a href="#ref-IMLDEA" role="doc-biblioref">2018</a>)</span>
The problem, however, turned out to be blackbox models, which did not provide sufficient information about the motivation in making specific decisions by the models.
‘’Machine Learning models have been branded as ‘Black Boxes’ by many. This means that though we can get accurate predictions from them, we cannot clearly explain or identify the logic behind these predictions.” <span class="citation">(Pandey <a href="#ref-IML" role="doc-biblioref">2019</a>)</span></p>
<p>Interpretability of models is a desirable feature among specialists in fields other than machine learning, it helps them make better decisions, justify their choices, and combine expert knowledge with the model’s indications.
‘’ Machines and humans work differently in how they sense, understand and learn. Machines are better at recognizing low-level patterns in huge amounts of data, while people excel at connecting the dots among high-level patterns. To make better decisions, we need both working together. ‘’ <span class="citation">(accenture <a href="#ref-UME" role="doc-biblioref">2018</a>)</span>
Trust and transparency are also demanded.</p>
<p>There are many methods that can help us create an interpretable model
‘’The easiest way to achieve interpretability is to use only a subset of algorithms that create interpretable models. Linear regression, logistic regression and the decision tree are commonly used interpretable models.’’ <span class="citation">(Molnar <a href="#ref-christophmonlar" role="doc-biblioref">2019</a>)</span></p>
<p>Another way may be to use blackboxes to create an interpretable model.
They can help us during transformation of the original data set or, for example, in selecting variables.
In this article, we will discuss the process of creating an interpretable model whose target effectiveness will be comparable to blackbox models. We will present the whole
workflow, during which we will get acquainted with the dataset with which we will work, we will use advanced feature engineering methods and compare the results obtained during all phases of process. An additional problem we will face during work will be unbalanced data and creating a model that will take them into account during prediction.</p>
<p>We will use machine learning tools and frameworks available in R and Python. The subject of our analysis will be the “Adult” data set - (<a href="https://www.openml.org/d/179?fbclid=IwAR3W2OO_QNLM9cLmThzwmtJjOZ-GteprmynSTumIcTyT93BVeTX4gbGbZtM" class="uri">https://www.openml.org/d/179?fbclid=IwAR3W2OO_QNLM9cLmThzwmtJjOZ-GteprmynSTumIcTyT93BVeTX4gbGbZtM</a>). The goal is to predict whether income exceeds 50 000 $
annually based on census data.</p>

</div>
</div>
<div id="which-neighbours-affected-the-price-of-a-house-in-the-90s" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Which Neighbours Affected the Price of a House in the ’90s?</h2>
<p><em>Authors: Hubert Baniecki, Mateusz Polakowski (Warsaw University of Technology)</em></p>
<!-- What Affects The Price Of A House? US Census Data Revisited After 30 Years.-->
<div id="introduction-2" class="section level3" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Introduction</h3>
<p>Real estate value varies over numerous factors. These may be obvious like location or interior design, but also less apparent like the ethnicity and age of neighbours. Therefore, property price estimation is a demanding job that often requires a lot of experience and market knowledge. Is or was, because nowadays, Artificial Intelligence (AI) surpasses humans in this task. Interested parties more often use tools like supervised Machine Learning (ML) models to precisely evaluate the property value and gain a competitive advantage.</p>
<p>The dilemma is in blindly trusting the prediction given by so-called black-box models. These are ML algorithms that take loads of various real estate data as input and return a house price estimation without giving their reasoning. Black-box complex nature is its biggest strength and weakness at the same time. This trait regularly entails high effectiveness but does not allow for interpretation of model outputs. Because of that, specialists interested in supporting their work with automated ML decision-making are more eager to use white-box models like linear regression or decision trees. These do not achieve state-of-the-art performance efficiently, but instead, provide valuable information about the relationships present in data through model interpretation.</p>
<p>For many years houses have been the most popular properties; thus, they are of particular interest for ordinary people. What exact influence had the demographic characteristics of the house neighbourhood on its price in the ’90s? Although in the absence of current technology, it has been hard to answer such question years ago, now we can.</p>
<p>In this paper, we perform a case study on the actual US. Census data from 1990 and deliver an interpretable white-box model that estimates the median house price by the region. We present multiple approaches to this problem and choose the best model, which achieves similar performance to complex black-boxes. Finally, using its interpretable nature, we answer various questions that give a new life to this historical data.</p>
<!-- Add citations -->
</div>
<div id="data" class="section level3" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Data</h3>
<!-- http://www.cs.toronto.edu/~delve/data/census-house/censusDetail.html -->
<p>Data description. Done but not polished.</p>
</div>
<div id="methodology-12" class="section level3" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> Methodology</h3>
<p>In this section, we are going to focus on developing the best white-box model predicting house prices median, which provides interpretability of features. Finally, we will compare these results with black-box models.</p>
</div>
<div id="results-12" class="section level3" number="3.6.4">
<h3><span class="header-section-number">3.6.4</span> Results</h3>
<p>Better than black-box?? Spoiler alert.</p>
</div>
<div id="conclusions" class="section level3" number="3.6.5">
<h3><span class="header-section-number">3.6.5</span> Conclusions</h3>
<p>Spoiler: Trees are cool. House prices are ethnically biased.</p>

</div>
</div>
<div id="explainable-computer-vision-with-embeddings-and-knn-classifier." class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Explainable Computer Vision with embeddings and KNN classifier.</h2>
<p><em>Authors: Olaf Werner, Bogdan Jastrzębski (Warsaw University of Technology)</em></p>
<div id="abstract-13" class="section level3" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Abstract</h3>
</div>
<div id="introduction-3" class="section level3" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Introduction</h3>
<p>Computer vision is widely known use case for neural networks. However neural networks are infamous for their complexity and lack of interpretability. On the other hand simple classifiers like KNN have really poor results for complex tasks like image recognition. In this article we will prove that it is possible to get best of both worlds using emmbeddings.</p>
</div>
<div id="data-1" class="section level3" number="3.7.3">
<h3><span class="header-section-number">3.7.3</span> Data</h3>
<p>We are going to use dataset <a href="https://www.openml.org/d/40996">Fashion-Mnist</a>. Fashion-MNIST is a dataset of Zalando’s article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Classes are following: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot.</p>
</div>
<div id="methodology-13" class="section level3" number="3.7.4">
<h3><span class="header-section-number">3.7.4</span> Methodology</h3>
</div>
<div id="results-13" class="section level3" number="3.7.5">
<h3><span class="header-section-number">3.7.5</span> Results</h3>
</div>
<div id="conclusions-1" class="section level3" number="3.7.6">
<h3><span class="header-section-number">3.7.6</span> Conclusions</h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-UME">
<p>accenture. 2018. “UNDERSTANDING Machines: EXPLAINABLE Ai,” 19. <a href="https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf?fbclid=IwAR0ZtyDNzHR8dMUJHPwa0CkuQXgOOE68UQV4JCcBxXudO3dlm14LjqX-B8g">https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf?fbclid=IwAR0ZtyDNzHR8dMUJHPwa0CkuQXgOOE68UQV4JCcBxXudO3dlm14LjqX-B8g</a>.</p>
</div>
<div id="ref-fenton97">
<p>Fenton, N. E., and S. L. Pfleeger. 1997. <em>Software Metrics: A Rigorous &amp; Practical Approach</em>. International Thompson Press.</p>
</div>
<div id="ref-mccabe76">
<p>McCabe, T. J. 1976. “A Complexity Measure.” <em>IEEE Transactions on Software Engineering</em> 2 (4): 308–20.</p>
</div>
<div id="ref-christophmonlar">
<p>Molnar, Christoph. 2019. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>. <a href="https://christophm.github.io/interpretable-ml-book/simple.html">https://christophm.github.io/interpretable-ml-book/simple.html</a>.</p>
</div>
<div id="ref-IMLDEA">
<p>Murdoch, Kumbier, Singh. 2018. “Interpretable Machine Learning: Definitions, Methods, and Applications,” 2. <a href="https://arxiv.org/pdf/1901.04592.pdf?fbclid=IwAR2frcHrhLc4iaH5-TmKKq263NVvAKHtG4uQoiVNDeLAG3QFzdje-yzZjiQ">https://arxiv.org/pdf/1901.04592.pdf?fbclid=IwAR2frcHrhLc4iaH5-TmKKq263NVvAKHtG4uQoiVNDeLAG3QFzdje-yzZjiQ</a>.</p>
</div>
<div id="ref-IML">
<p>Pandey. 2019. “Interpretable Machine Learning: Extracting Human Understandable Insights from Any Machine Learning Model,” April. <a href="https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b">https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b</a>.</p>
</div>
<div id="ref-Sayyad-Shirabad+Menzies:2005">
<p>Sayyad Shirabad, J., and T. J. Menzies. 2005. “The PROMISE Repository of Software Engineering Databases.” School of Information Technology and Engineering, University of Ottawa, Canada. <a href="http://promise.site.uottawa.ca/SERepository">http://promise.site.uottawa.ca/SERepository</a>.</p>
</div>
<div id="ref-OpenML2013">
<p>Vanschoren, Joaquin, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. 2013. “OpenML: Networked Science in Machine Learning.” <em>SIGKDD Explorations</em> 15 (2): 49–60. <a href="https://doi.org/10.1145/2641190.2641198">https://doi.org/10.1145/2641190.2641198</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="imputation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="acknowledgements.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mini-pw/2020L-WB-Book/edit/master/3-0-interpretability.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

</body>

</html>
